{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Available GPUs: {num_gpus}\")\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    print(f\"Device ID: {i}, GPU Name: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# hf_access_token = os.environ.get('HF_ACCESS_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'HuggingFaceH4/zephyr-7b-alpha'\n",
    "model_path='/media/d1/huggingface.co/models/meta-llama/Llama-2-7b-chat-hf/'\n",
    "# model_path='meta-llama/Llama-2-7b-chat-hf' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79241fc7aa84175bc162cc1e41bd5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FIXME: Should be float16 instead of float32\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32, device_map=\"auto\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32, device_map=\"auto\", token=hf_access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Please list 17 U.S. politicians that have been active since 1980. Format the response as a simple, un-numbered comma separated list with only full names.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"List 20 famous U.S. politicians that also have a famous blood relative in politics. List only their names. Only include cases where both the politician and the corresponding relative have had political careers that have been active since the 1980s. Only include cases where both the politician and the corresponding relative have the same surname.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'List 20 famous U.S. politicians that also have a famous blood relative in politics. List only their names. Only include cases where both the politician and the corresponding relative have had political careers that have been active since the 1980s. Only include cases where both the politician and the corresponding relative have the same surname.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt:\n",
      "For each politicians returned in the list above, please list a corresponding blood relative that is also in politics.\n",
      "\n",
      "Response:\n",
      "For example, for Angela Merkel, her brother Horst Merkel is also in politics.\n",
      "\n",
      "Note: This list only includes politicians who are directly related to the politicians listed above (i.e. siblings, parents, children).\n",
      "\n",
      "1. Angela Merkel - Horst Merkel (brother)\n",
      "2. Emmanuel Macron - None\n",
      "3. Justin Trudeau - None\n",
      "4. Boris Johnson - None\n",
      "5. Scott Morrison - None\n",
      "6. Jair Bolsonaro - None\n",
      "7. Narendra Modi - None\n",
      "8. Xi Jinping - None\n",
      "9. Vladimir Putin - None\n",
      "10. Recep Tayyip Erdogan - None\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    response_id = model.generate(**model_input, max_length=model_input.input_ids.size(1) + 500, temperature=0.4, do_sample=True)\n",
    "\n",
    "# Extract and display the response\n",
    "response_text = tokenizer.decode(response_id[0], skip_special_tokens=True)[len(tokenizer.decode(model_input.input_ids[0], skip_special_tokens=True)):]\n",
    "print(\"User Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from entity_extraction import EntityExtraction\n",
    "\n",
    "def get_political_names(prompt, minResults=5, retryCnt=3):\n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        response_id = model.generate(**model_input, max_length=model_input.input_ids.size(1) + 500, temperature=0.4, do_sample=True)\n",
    "\n",
    "    # Extract and display the response\n",
    "    response_text = tokenizer.decode(response_id[0], skip_special_tokens=True)[len(tokenizer.decode(model_input.input_ids[0], skip_special_tokens=True)):]\n",
    "    extractor = EntityExtraction()\n",
    "\n",
    "    names = []\n",
    "    while True:\n",
    "        names = extractor.get_full_names(response_text)\n",
    "        if len(names) < minResults and retryCnt > 0:\n",
    "            retryCnt =- 1\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    return names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names:  ['Joe Biden', 'John F. Kennedy', 'Robert F. Kennedy', 'Ted Kennedy', 'Barack Obama', 'Bill Clinton', 'Hillary Clinton', 'George H.W', 'Bush', 'George W. Bush', 'Jeb Bush', 'Jimmy Carter', 'Rosalynn Carter', 'Al Gore', 'Tipper Gore', 'John McCain', 'Meghan McCain', 'Lindsey Graham', 'Bob Graham', 'Michelle Obama']\n"
     ]
    }
   ],
   "source": [
    "names = get_political_names(prompt)\n",
    "print(\"names: \", names)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"Angela Merkel\"\n",
      "name: \"Horst Merkel\"\n",
      "name: \"Angela Merkel\"\n",
      "name: \"Horst Merkel\"\n",
      "name: \"Emmanuel Macron\"\n",
      "name: \"Justin Trudeau\"\n",
      "name: \"Boris Johnson\"\n",
      "name: \"Scott Morrison\"\n",
      "name: \"Jair Bolsonaro\"\n",
      "name: \"Narendra Modi\"\n",
      "name: \"Vladimir Putin\"\n",
      "name: \"Recep Tayyip Erdogan\"\n",
      "names:  ['Angela Merkel', 'Horst Merkel', 'Angela Merkel', 'Horst Merkel', 'Emmanuel Macron', 'Justin Trudeau', 'Boris Johnson', 'Scott Morrison', 'Jair Bolsonaro', 'Narendra Modi', 'Vladimir Putin', 'Recep Tayyip Erdogan']\n"
     ]
    }
   ],
   "source": [
    "# from textattack.nlp import EntityExtraction\n",
    "from entity_extraction import EntityExtraction\n",
    "\n",
    "\n",
    "extractor = EntityExtraction()\n",
    "names = extractor.get_full_names(text=response_text)\n",
    "if len(names) > 0:\n",
    "    for name in names:\n",
    "        name = ' '.join(name.split())\n",
    "        print(f\"name: \\\"{name}\\\"\")\n",
    "        # print(name)\n",
    "\n",
    "\n",
    "print(\"names: \", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"For each politicians returned in the list above, please list a corresponding blood relative that is also in politics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt:\n",
      "For each politicians returned in the list above, please list a corresponding blood relative that is also in politics.\n",
      "\n",
      "Response:\n",
      "For example, for Angela Merkel, her brother Horst Merkel is also in politics.\n",
      "\n",
      "Note: I will only consider blood relatives, not spouses or partners.\n"
     ]
    }
   ],
   "source": [
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    response_id = model.generate(**model_input, max_length=model_input.input_ids.size(1) + 500, temperature=0.4, do_sample=True)\n",
    "\n",
    "# Extract and display the response\n",
    "response_text = tokenizer.decode(response_id[0], skip_special_tokens=True)[len(tokenizer.decode(model_input.input_ids[0], skip_special_tokens=True)):]\n",
    "print(\"User Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(response_text.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextAttackDev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
